>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 9          |
|-----------------             |
| Student 1    Maitry Rohit    |   
| Student 2    Sobia Khan      |   
| Student 3    Christina Wyllie|   
| Student 4    Jamie Stade     |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

In this lab, we performed tests on a mock ATM application, with the purpose of learning about exploratory, manual, and regression testing, as well as how to use bug tracking software. Before performing the various tests on the application, we did not know exactly what was expected from each type of testing method. We knew exploratory entailed executing unscripted tests to see how the system performs, however, we learned that in order to do this section effectively, we should first develop a sort of plan. Creating a list of functionalities a system should be able to perform, based on some given requirements, allows exploring the system to be done in a much more organized fashion. Additionally, we knew that manual testing consists of testers manually executing a series of scripted tests without automating the process. This method is more straightforward in its execution, as the lab document contains a list of test cases for the system, so we know exactly what needs to be tested in this method.

# High-level description of the exploratory testing plan

To conduct exploratory tests on the program, the group of four will be split into two pairs of two members. Pair one is going to first perform tests on the Main Menu page, testing for the ATM turning on and off and each of the buttons such as “Clear”, “Enter”, and “Cancel”. They will also test functionalities when logging in with both cards. This is done to ensure the buttons are working as per the specified requirements (such as “can cancel at any time”).  To split up the subsequent work, pair 1 is going to perform tests on the functionalities “Withdraw” and “Balance Inquiry”. To test “Withdraw”, each of the accounts will be tested for each card and the different amount of cash withdrawn will be explored. They will also perform tests on “Balance Inquiry”, checking for the correct balance being displayed for each account that the card has access to. For both of these transactions being tested, they will look at the receipt and log entries that are produced. Pair two is going to perform the same tests on the main menu, looking for anything the other group did not find. They will primarily focus on “Deposit” and “Transfer”. The “Deposit” functionality is tested, looking for the proper amount to be deposited and to the correct account. The “Transfer” functionality is going to look for the correct amount of money being transferred to the requested account. These main functions are being tested to ensure the base functionality is working properly in order to fulfill the requirements specified. Additionally, the team plans to explore the functionalities on each page to ensure no specific bugs occur. For example, a bug may occur when selecting a checking account within withdrawal, but not the savings account. After each of the main functionalities are tested, the pair are going to compare results found and then swap the transactions they performed tests on to make sure no errors were missed. As a last step, both pairs are going to look for any bugs that are mentioned as requirements in the lab document, but which are not attached to a specific functionality, such as pressing a number higher than the list of options (eg. 4 options displayed, what happens when 5 is pressed). 

# Comparison of exploratory and manual functional testing

Both exploratory and manual testing are useful methods to find bugs in code. However, both have their respective advantages and disadvantages. 

The manual scripted testing suite was far more detailed and specific about each test case. Every case was explicitly explained and well thought out. Comparatively, the exploratory testing plan made by the team was far more high-level and generalized. Overall, manual scripted testing can be seen as more appealing because there is a specific set of guidelines, whereas exploratory testing was far more disorganized in both searching for bugs and writing them down. Having a list of functionalities to test ensures that important functionalities, such as the user correctly inputting the PIN to gain access to their account, work completely and are free of bugs. The disadvantage we found from manual functional testing is that it takes longer than exploratory testing because you must first make a descriptive plan, which provides details on how to perform each test. Manual testing was more straightforward and in-depth, but exploratory testing was more efficient. 

With this specific test suite, the exploratory testing method was found to be far more exhaustive than manual testing. Despite testing every main functionality, manual testing did not consider many smaller specific inputs or details. For example, the manual functional testing had no requirements for the displays to have the correct spelling for all words, which is one of the defects we found during our exploratory testing phase. Another defect found that was a part of the SUT Use Cases and not in the manual testing was whether a number on the keypad pressed that was not part of the menu options would do anything. For example, if the menu had options 1 - 5, what would happen if 7 were chosen? This happened to produce a bug that the scripted testing suite did not consider because it was not a main functionality. Even some main functionalities, such as whether the money market and savings accounts were properly displayed were not tested during the manual scripted testing phase. 

From our experience, exploratory testing was more effective than manual scripted testing and caught bugs that would have otherwise not have been found if just scripted testing was conducted. 

# Notes and discussion of the peer reviews of defect reports

Peer reviews were done for both the exploratory and scripted testing. The group discussed a high level exploratory testing plan together and shared ideas for which functions need to be tested. Teams would individually conduct tests (breakdown given below) and place them into Jira using the given formatting in the lab document. Other members of the team who didn’t see that error would then attempt to reproduce it, using the steps outlined in the Jira log, and then add other relevant information if it was missing. For example, one member would write down the error and the other would attempt to reproduce the visual and put in a screenshot of the bug confirming it exists. During exploratory testing, each pair would update the other on any bugs found in the program to ensure no duplicate bugs were found. Any bugs that one team found that another didn’t, were checked by the other team to make sure the error exists. Every member then reviewed all the bugs written and would add extra information to ensure a detailed explanation and breakdown of the bug was written so that future engineers may reproduce any errors for this program. For version 1.1 of the program, the team then reviewed all the bugs and worked together to retest the bugs and note down any new defects or existing defects.

# How the pair testing was managed and team work/effort was divided 

The team of four was divided into two; Sobia and Jamie as one team and Maitry and Tina as the other. Each team conducted pair testing together for the initial exploratory testing. The group went through the high level exploratory testing plan with one person running the application and the other writing specific bugs down and keeping track of tests left to do. The team decided to use Jira to track bugs found within the application. Both teams placed any bugs they found that the other team hadn’t already found in the Jira document. Scripted testing was then done by all 4 members where one member would go through the tests on the application and screenshot any defects, 2 others would write down the errors in Jira, and another would track what test has been completed and what to do for each test. Each member contributed to the lab document and wrote down certain sections. For regression testing, the same type of work division was used. One person went through the list of existing bugs reported in the Jira software and told another person to perform the exact steps to produce the bug, the third member would mark if this bug was “done” or “in progress” and the last person would edit the screenshots provided by the person who found the bug. Overall, we were able to work as a team to make the process of testing more efficient and to ensure every test was performed.

# Difficulties encountered, challenges overcome, and lessons learned

The difficulties we encountered while working on this lab were scheduling a time when everyone could work on the assignment together. Another difficulty we encountered was timing since the assignment was released later in the week than anticipated. We had less time to complete the lab than we originally planned and this also made it more difficult to schedule time to work on the assignment as a group. Learning how to use a new software with very little guidance was also a difficulty we encountered.

The challenges we overcame were learning to use Jira to track bugs, despite never using it before, and working within the time constraints to finish the lab report and all lab requirements with the correct formatting. These challenges were dealt with as a team and we successfully learned how to use the Jira software to report bugs. Additionally, we were able to learn how to use a markdown file to create this lab report and how to create our github to hand in our work.

The lessons we learned from performing this lab were how to conduct exploratory testing on a software system by testing all the system requirements, how to conduct manual scripted tests, and how to conduct regression testing on an updated version of a system. We also learned that we should discuss each team member’s availability to find a time when we can consistently meet outside of the designated lab time. We also learned that working as a team makes finishing tasks both quicker and slower. It is slower because we have to find a time to work that works with everyone’s schedules, however, working in a group allows you to divide up tasks and have people work on different tasks at the same time. Additionally, being able to discuss ideas with everyone allows us to learn from each other.

# Comments/feedback on the lab and lab document itself

A thorough explanation of the parts we needed to complete for the lab by the TA’s would’ve been helpful in understanding how to complete the lab. As a team, we were left generally confused trying to figure out what exactly we were trying to achieve as the formatting of the .md file and the explanations given were slightly confusing. For example, if the process was explained to us beforehand, the document would have made a little more sense, or, a small explanation for how exploratory testing could be conducted would have been helpful when first starting this lab. The .md file contained a lot of information at once and was found to be overwhelming. Writing the lab report in a .md format rather than a .pdf format was also tough since we had never seen this formatting before. An explanation for how we can use it would’ve allowed for the reporting process to be conducted more smoothly. When introducing new technology or formatting styles, the submission process would be made easier if a quick verbal overview of the process to submit and create the requested files was given to the class.
